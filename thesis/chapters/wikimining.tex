\chapter{WikiMining framework design}

The framework is in an \emph{alpha} development state. While all the described
functionality is implemented and works accordingly, you might sometimes find
yourself needing to change the code in order to tweak some of the behaviour or
that it might not be straight-forward to extend the framework with some
specific complex functionality. However, I intend to bring it to a quality
standard that will allow you to perform both of the above tasks with ease.

\section{External libraries}

Dummy text.

\subsection{Java Wikipedia Library}

\ac{JWPL} is an open-source library that facilitates access to a lot of
information concerning Wikipedia including, but not limited to:
\begin{itemize}
  \item page link graph
  \item category link graph
  \item page \(\leftrightarrow\) category link graph
  \item page content (MediaWiki and plain-text formats)
\end{itemize}

\paragraph{How it works?} Given a Wikipedia dump - that can be downloaded from
\todo{http://dumps.wikimedia.org/} - it generates \(11\) text files that can
then be imported into a MySQL databased and accessed through the \ac{JWPL}
\ac{API}. You can find more details about using \ac{JWPL} on their website
\todo{https://code.google.com/p/jwpl/wiki/DataMachine}.

The library is publicly available \todo{https://code.google.com/p/jwpl/} and
its authors wrote a paper \cite{zesch2008jwpl} providing further insights into
its inner workings. For the purposes of this thesis we use \emph{\ac{JWPL}
0.9.2}.

\subsection{Hadoop}

\emph{Hadoop} is an open-source project that aims to offer solutions for
'reliable, scalable, distributed computing' \todo{http://hadoop.apache.org/}.
Although the whole system is more complex we mainly use two of its components:
\begin{itemize}
  \item \acl{MR}
  \item \acl{HDFS}
\end{itemize}

\subsubsection{MapReduce}

\acf{MR} is a two-phase process that borrows two concepts from functional
programming:
\begin{itemize}
  \item map;
  \item reduce;
\end{itemize}
and applies them to distributed computing.
The main steps are as follows:
\begin{enumerate}
  \item In the first stage - the map stage - the data is partitioned and spread
  across multiple processes that perform the same task and output multiple
  (key, value) pairs.
  \item In-between the two phases, the results from the first stage are
  collected and sorted by key in a distributed fashion.
  \item In the second stage - the reduce stage - all pairs that share the same
  key are sent to the same process and this task outputs a results based on the
  received pairs (that share the same key).
\end{enumerate}
\acf{MR} is a very powerful technique for doing distributed computations
because it provides an easy way to write distributed code that is both
fault-tolerant and free from other distributed computing problems (such as
synchronisation concerns, deadlocks etc.).
If you are interested in learning more about \acf{MR} we recommend reading the
paper \cite{dean2008mapreduce} written by its proponents.
\missingfigure{MapReduce stages}

\subsubsection{Hadoop Distributed File System}

\acf{HDFS} is, as it name says, a distributed file-system that offers the
file-system back-end for writing \acl{MR} jobs and using other Hadoop-based
frameworks. Its purpose is to store large-scale data reliably and
distributively such that it can be easy to stream this data at a high bandwidth
to applications running on multiple machines \cite{shvachko2010hadoop}.

For the purposes of this thesis we use \emph{Hadoop 1.0.4}.

\subsection{Mahout}

\emph{Mahout} is a scalable machine learning library designed to run over very
large data sets using Hadoop \acl{MR}. While it offers distributed algorithms
for a lot of machine learning problems such as:
\begin{itemize}
  \item classification
  \item clustering
  \item recommendations
\end{itemize}
we are only using Mahout for indexing Wikipedia and creating the corresponding
tf-idf vectors.

\missingfigure{Architecture from http://www.slideshare.net/VaradMeru/introduction-to-mahout-and-machine-learning ?}

\paragraph{Generating tf-idf vectors} To create tf-idf vectors from a set of
plain-text documents \footnote{Stored in a \ac{HDFS}-specific format called
Sequence Files.} is being done using \emph{mahout seq2sparse}. This tool has
parameters that allow us to filter out rare words, stop-words and even perform
stemming if needed.

If you are interested in finding out more about Mahout you can read the book
\emph{Mahout in Action} \todo{Citation needed} or visit their website
\todo{https://mahout.apache.org/}. For the purposes of this thesis we use
\emph{Mahout 0.9}.

\textbf{Cloud9}\todo{Used for XML -> plain conversion}

\section{System architecture}

In this section we present how we designed and implemented the WikiMining
library for summarising Wikipedia using submodular function maximisation.

\subsection{Base data types}

We use the following data type classes, defined in package
\emph{ch.ethz.las.wikimining.base}:
\begin{description}
  \item[ArrayWritable] Different subclasses for serialiasing an array of
  different types:
    \begin{itemize}
      \item \emph{IntArrayWritable} - for integers: \emph{IntWritable};
      \item \emph{TextArrayWritable} - for strings: \emph{Text};
    \end{itemize}
  or offer more functionality such as print its content in plain text using a
  \emph{toString()} method.
  \item[DocumentWithVector] An (Integer document id, Vector) pair used in a lot
  of MapReduces to partition the data as part of:
    \begin{itemize}
      \item the GreeDi protocol (see Section~\vref{sec:submod-max});
      \item \acl{LSH} (see Section~ \todo{Preliminaries}).
    \end{itemize}
    It also comes in a \emph{Writable} form so that it can be serialisable by
    Hadoop \acl{MR}.
  \item[HashBandWritable] A (hash, band) pair used for \ac{LSH}.
  \item[SquaredNearestNeighbour] A collection that gets the nearest neighbour
  of an article (based on cosine similarity) from among the documents that were
  written before the input article. This is used to compute \emph{document
  influence}.
  \item[Vector] Class from Mahout that represents a \emph{double}-valued
  mathematical vector. It comes in two flavours:
  \begin{itemize}
    \item \emph{DenseVector} - stored as a normal array
    \item \emph{RandomAccessSparseVector} - stored as a hash-map of indexes to
    values.
  \end{itemize}
\end{description}

In addition we have two classes for:
\begin{itemize}
  \item storing the default parameters - \emph{Defaults}
  \item defining the different program arguments - \emph{Fields}
\end{itemize}

\subsection{Submodular functions}

Dummy text.

\subsection{Submodular function maximisation}

Dummy text.

\subsection{Coverage algorithms}

Dummy text.

\subsection{Influence algorithms}

Dummy text.

\subsection{Selection and evaluation}

Dummy text.

\subsection{Input, output}

Dummy text.

