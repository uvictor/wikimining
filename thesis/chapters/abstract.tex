\begin{abstract}
As a result of the volume of content that exists today on the internet, it has
become increasingly harder for content creators and consumers alike to manage
this content in a centralised manner. One such example is Wikipedia which is
very hard to analyse and digest at a general level, mainly because of its
prohibitive large size. We aim to discover what are the most important
Wikipedia articles and how articles evolve in popularity and influence over
time. At a smaller scale, we investigate these same problems on specific
categories.

Attempts to analyse Wikipedia have been made in the past using quantitative
measures and semantic coverage, but these studies were done back when Wikipedia
was a lot smaller than it is today and none of these papers deals with
detecting influential articles (using unsupervised methods). More often
Wikipedia was used in natural language processing, especially for computing
semantic relatedness based on the Wikipedia categories, but these papers deal
only indirectly with analysing Wikipedia itself.

In order to study the evolution of Wikipedia’s network of articles and find
representative subsets, we investigate different solutions using submodular
function maximisation. We adapt existing submodular functions from the
literature and define new functions to solve this problem. We devise a
framework -- WikiMining -- that can scale up to Wikipedia’s size, using the
Distributed Submodular Maximisation (GreeDi) protocol.
\end{abstract}
