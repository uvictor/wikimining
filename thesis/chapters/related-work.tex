\chapter{Related work}

As presented in the introduction, our topic relates to two main fields of
interest: one is analysing Wikipedia and the other is multi-document
summarisation. Most Wikipedia analysis are quantitative and deal with looking
at the pages from a data analysis perspective. On the other side,
multi-document summarisation is more related to our work, but deals with
smaller sets of documents and has a different objective.

On the topic of \emph{data analysis}, Voss \cite{voss2005measuring} does an
excellent job at looking at Wikipedia's growth, articles and meta-pages
distribution, article sizes distribution, authors' statistics and graph
structure in multiple languages (English, German, Japanese and others). This is
a quantitative analysis, helpful to better understand Wikipedia's structure and
find possible submodular function that can capture informativeness.
On the same topic, but looking into Wikipedia's \emph{semantic coverage},
Holloway et al \cite{holloway2007analyzing} analyse Wikipedia based on the
semantic structures one can discover from the articles' graphs, categories'
interlinkage and revisions statistics.
More closely related to natural language processing, but further away our
interest of multi-document summarisation, Gabrilovich et al
\cite{gabrilovich2007computing} look into \emph{explicit semantic analysis} to
discover the meaning of the words using Wikipedia.

On the topic of multi-document summarisation using submodular function
maximisation, an important reference that provided us with a starting point is
the paper of \cite{sipos2012temporal} about using the expansion of
\emph{document influence} over time in order to capture papers' importance.
This method has encouraging results on conferences' corpora of published
papers, but does not manage to transfer its results to a massive corpora like
Wikipedia.

As far as we know there has not been any previous work on summarising massive
corpora using submodular function maximisation.

